{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9012182b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook benchmarks PyTorch's scaled_dot_product_attention (SDPA) against a vLLMs Triton-based flash attention kernel.\n",
    "\n",
    "Key highlights:\n",
    "- Environment Setup: GPU checks and Triton installation.\n",
    "- Baseline Performance: Measure PyTorch SDPA runtimes for various sequence lengths.\n",
    "- vLLM Triton Kernel Benchmark: Compare initial vLLM kernel performance vs. PyTorch.\n",
    "  - Triton Autotuning & Caching:\n",
    "    - The first run triggers autotuning (testing multiple configurations), making it slower.\n",
    "    - The best configuration is cached for future runs.\n",
    "    - Subsequent runs reuse the cached kernel and run significantly faster without re-tuning.\n",
    "- Visualization: Clear plots show performance improvements before and after autotuning.\n",
    "- Speedup Summary: A table and plots demonstrate consistent 2-4x speedups compared to PyTorch after caching.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29482448-d69e-4412-82b8-e6b8243699fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python triton-gpu-check.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9012182b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook benchmarks PyTorch's scaled_dot_product_attention (SDPA) against a vLLMs Triton-based flash attention kernel.\n",
    "\n",
    "Key highlights:\n",
    "- Environment Setup: GPU checks and Triton installation.\n",
    "- Baseline Performance: Measure PyTorch SDPA runtimes for various sequence lengths.\n",
    "- vLLM Triton Kernel Benchmark: Compare initial vLLM kernel performance vs. PyTorch.\n",
    "  - Triton Autotuning & Caching:\n",
    "    - The first run triggers autotuning (testing multiple configurations), making it slower.\n",
    "    - The best configuration is cached for future runs.\n",
    "    - Subsequent runs reuse the cached kernel and run significantly faster without re-tuning.\n",
    "- Visualization: Clear plots show performance improvements before and after autotuning.\n",
    "- Speedup Summary: A table and plots demonstrate consistent 2-4x speedups compared to PyTorch after caching.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29482448-d69e-4412-82b8-e6b8243699fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python triton-gpu-check.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591e3d51-8a52-45b5-b6a8-99acd0b1180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd triton && pip install ./python && cd -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2278ff2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Triton version:\", triton.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bc884d",
   "metadata": {},
   "source": [
    "## Flash Attention Benchmark (PyTorch SDPA vs vLLM Kernel)\n",
    "This notebook benchmarks the PyTorch `scaled_dot_product_attention` against the vLLM Triton-based flash attention kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vllm-import",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming vllm_flash_attention.py is present in the same directory or accessible path\n",
    "from flash_attention import triton_attention as vllm_flash_attention\n",
    "from flash_attention import benchmark_flash_attention as vllm_benchmark\n",
    "from flash_attention import attn_fwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0bbe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pytorch_sdpa(q, k, v):\n",
    "    return torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac54ef2-2877-49bb-aca6-c7ca2e6d55d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /workspace/.triton/cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vllm-kernel-wrapper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vllm_flash_attention(q, k, v, seqlen):\n",
    "    q_flat = q.permute(0, 2, 1, 3).reshape(-1, q.shape[1], q.shape[3])\n",
    "    k_flat = k.permute(0, 2, 1, 3).reshape(-1, k.shape[1], k.shape[3])\n",
    "    v_flat = v.permute(0, 2, 1, 3).reshape(-1, v.shape[1], v.shape[3])\n",
    "    cu_seqlens_q = torch.arange(0, q.shape[0] + 1, dtype=torch.int32, device=q.device) * seqlen\n",
    "    cu_seqlens_k = torch.arange(0, q.shape[0] + 1, dtype=torch.int32, device=q.device) * seqlen\n",
    "    o, _ = vllm_flash_attention(q_flat, k_flat, v_flat, None, cu_seqlens_q, cu_seqlens_k, seqlen, seqlen, False, 1.0, None)\n",
    "    return o.view(q.shape[0], seqlen, q.shape[1], q.shape[3]).permute(0, 2, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b911bed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_flash_attention(batch, nheads, head_dim, seqlen):\n",
    "    q = torch.randn(batch, nheads, seqlen, head_dim, device='cuda')\n",
    "    k = torch.randn(batch, nheads, seqlen, head_dim, device='cuda')\n",
    "    v = torch.randn(batch, nheads, seqlen, head_dim, device='cuda')\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    out_torch = run_pytorch_sdpa(q, k, v)\n",
    "    torch.cuda.synchronize()\n",
    "    pytorch_time = time.time() - start\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    out_vllm = run_vllm_flash_attention(q, k, v, seqlen)\n",
    "    torch.cuda.synchronize()\n",
    "    vllm_time = time.time() - start\n",
    "\n",
    "    diff_vllm = torch.max(torch.abs(out_torch - out_vllm)).item()\n",
    "    return pytorch_time, vllm_time, diff_vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1ce123",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlens = [128, 256, 512, 1024]\n",
    "batch, nheads, head_dim = 32, 8, 64\n",
    "pytorch_times, vllm_times, vllm_diffs = [], [], []\n",
    "\n",
    "for seqlen in seqlens:\n",
    "    t_pt, t_vllm, d_vllm = benchmark_flash_attention(batch, nheads, head_dim, seqlen)\n",
    "    pytorch_times.append(t_pt)\n",
    "    vllm_times.append(t_vllm)\n",
    "    vllm_diffs.append(d_vllm)\n",
    "    print(f\"Seqlen={seqlen}: PyTorch CUDA={t_pt:.4f}s, vLLM CUDA={t_vllm:.4f}s, Diff(vLLM)={d_vllm:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99804e7-4693-445e-a473-e2c243f77f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /workspace/.triton/cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6343046c-03ef-4326-a8b2-fbe33488a302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "def print_triton_cache_info_from_json():\n",
    "    \"\"\"\n",
    "    Scans the Triton cache directory, reads all JSON metadata files,\n",
    "    groups multiple kernels under the same JSON file name by printing distinct properties,\n",
    "    and exports CSV/Markdown/HTML reports.\n",
    "    \"\"\"\n",
    "    triton_cache_dir = Path(os.getenv('TRITON_CACHE_DIR', Path.home() / '.triton'))\n",
    "\n",
    "    if not triton_cache_dir.exists():\n",
    "        print(f\"Triton cache directory not found at {triton_cache_dir}\")\n",
    "        return\n",
    "\n",
    "    json_files = list(triton_cache_dir.glob('**/*.json'))\n",
    "\n",
    "    if not json_files:\n",
    "        print(\"No kernel metadata (.json) files found.\")\n",
    "        return\n",
    "\n",
    "    table_data = []\n",
    "\n",
    "    for json_file in json_files:\n",
    "        if json_file.name.startswith(\"__grp__\"):\n",
    "            continue  # Skip group metadata files\n",
    "\n",
    "        try:\n",
    "            with open(json_file, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "\n",
    "            so_file = json_file.with_suffix('.so')\n",
    "            so_exists = so_file.exists()\n",
    "\n",
    "            row = {\n",
    "                \"JSON File\": json_file.name,\n",
    "                \"Created\": time.ctime(json_file.stat().st_ctime),\n",
    "                \"Binary?\": \"Yes\" if so_exists else \"No\",\n",
    "                \"Binary Size (KB)\": (so_file.stat().st_size / 1024) if so_exists else None,\n",
    "                \"Kernel Name\": metadata.get(\"name\", \"-\"),\n",
    "                \"Device\": metadata.get(\"device\", \"-\"),\n",
    "                \"Cache Key\": metadata.get(\"cache_key\", \"-\"),\n",
    "                \"Signature\": metadata.get(\"signature\", \"-\"),\n",
    "                \"Num Warps\": metadata.get(\"num_warps\", \"-\"),\n",
    "                \"Num Stages\": metadata.get(\"num_stages\", \"-\"),\n",
    "            }\n",
    "\n",
    "            table_data.append(row)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {json_file}: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(table_data)\n",
    "\n",
    "    # Show multiple distinct kernels by keeping all rows and sorting\n",
    "    df[\"Created TS\"] = pd.to_datetime(df[\"Created\"])\n",
    "    df = df.sort_values(\"Created TS\", ascending=False).drop(columns=[\"Created TS\"])\n",
    "\n",
    "    print(tabulate(df, headers=\"keys\", tablefmt=\"fancy_grid\", showindex=False))\n",
    "\n",
    "    # Export formats\n",
    "    output_dir = triton_cache_dir / \"cache_report\"\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    df.to_csv(output_dir / \"triton_cache_report.csv\", index=False)\n",
    "    df.to_markdown(output_dir / \"triton_cache_report.md\", index=False)\n",
    "    df.to_html(output_dir / \"triton_cache_report.html\", index=False)\n",
    "\n",
    "    print(f\"\\nReports saved to: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de521f7f-d41e-4f91-a322-1017af31b33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_triton_cache_info_from_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8fe26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(seqlens, pytorch_times, label=\"PyTorch SDPA (CUDA)\")\n",
    "plt.plot(seqlens, vllm_times, label=\"vLLM Flash Attention (CUDA)\")\n",
    "plt.xlabel(\"Sequence Length\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.title(\"Flash Attention Performance: PyTorch vs vLLM on CUDA\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212e60a2",
   "metadata": {},
   "source": [
    "## What is Triton Autotuning?\n",
    "Triton allows kernels to be **autotuned**, meaning it will try multiple kernel configurations (block sizes, warp counts, pipeline stages) to find the optimal setup for your specific GPU hardware and workload shape.\n",
    "\n",
    "This autotuning process significantly improves performance and ensures the kernel is utilizing the GPU most efficiently.\n",
    "\n",
    "**How does it work?**  \n",
    "- Triton runs benchmarks internally with different configurations.  \n",
    "- It measures which configurations are fastest.  \n",
    "- The result is cached, so future runs use the best-found setup.\n",
    "\n",
    "**Why do we re-run tuning?**  \n",
    "- Hardware setups or driver versions may change.  \n",
    "- Workload shapes (sequence lengths, batch sizes) might differ from defaults.  \n",
    "- We want to confirm we’re using the best configuration for *this exact benchmark*.\n",
    "\n",
    "In the next cell, we trigger this autotuning pass.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ef7113",
   "metadata": {},
   "source": [
    "## Note on Triton Autotuning and Caching Example\n",
    "\n",
    "- On the **first run**, when a specific kernel configuration (based on GPU hardware, batch size, sequence length, and head dimensions) is encountered for the first time, **Triton triggers autotuning**.  \n",
    "   - This process tries multiple kernel configurations in the background and picks the fastest one.\n",
    "   - As a result, the **first run may be significantly slower** due to this tuning process.\n",
    "\n",
    "- Once the best-performing configuration is found, it is **stored in Triton's cache** (typically in `/workspace/.triton/cache`).\n",
    "\n",
    "- On **subsequent runs** with the same input shape and environment:\n",
    "   - Triton **loads the tuned configuration from cache** and skips tuning.\n",
    "   - This leads to **consistently fast kernel launches and execution** without re-tuning overhead.\n",
    "\n",
    "-  If you clear the cache, the next run will re-trigger autotuning.\n",
    "\n",
    "> In short:  \n",
    "> - First run = autotuning + execution (slow but smart)  \n",
    "> - All future runs = cached config + execution (fast and efficient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f972971b-fef9-4814-926a-da87432b47bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger re-tuning (will reuse cached or search if needed)\n",
    "vllm_benchmark.run(show_plots=False, print_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10b97b8-71b2-4dc8-ba84-78680631a58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal script for autotuning\n",
    "import torch\n",
    "\n",
    "batch = 2\n",
    "nheads = 2\n",
    "head_dim_options = [64, 128]\n",
    "seqlen = 64\n",
    "\n",
    "for head_dim in head_dim_options:\n",
    "    for causal in [False, True]:\n",
    "        for dropout_p in [0.0, 0.1]:\n",
    "            total_tokens = batch * seqlen\n",
    "            q = torch.randn((total_tokens, nheads, head_dim), device='cuda', dtype=torch.float16)\n",
    "            k = torch.randn_like(q)\n",
    "            v = torch.randn_like(q)\n",
    "            o = torch.empty_like(q)\n",
    "            cu_seqlens_q = torch.arange(0, batch + 1, device='cuda', dtype=torch.int32) * seqlen\n",
    "            cu_seqlens_k = torch.arange(0, batch + 1, device='cuda', dtype=torch.int32) * seqlen\n",
    "\n",
    "            # This calls attn_fwd under the hood and triggers autotune\n",
    "            _ = vllm_flash_attention(q, k, v, o, cu_seqlens_q, cu_seqlens_k, seqlen, seqlen, causal, 1.0, None)\n",
    "\n",
    "# Now inspect:\n",
    "for key, config in attn_fwd.cache.items():\n",
    "    print(f\"Best config for {key}: {config.kwargs}, num_warps={config.num_warps}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17649351-52ab-4553-a207-4c17183d8e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_autotune_cache(kernel, kernel_name=\"\"):\n",
    "    if not hasattr(kernel, 'cache') or not kernel.cache:\n",
    "        print(f\"{kernel_name or 'This kernel'} does not have autotuning results.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n=== Autotune Cache (Best Configs) for {kernel_name or kernel.__name__} ===\")\n",
    "    for key, config in kernel.cache.items():\n",
    "        print(f\"Key: {key}  ->  Best Config: {config.kwargs}, num_warps: {config.num_warps}\")\n",
    "\n",
    "    if hasattr(kernel, '_benchmarked_configs') and kernel._benchmarked_configs:\n",
    "        print(\"\\n=== Benchmarked Configs (all candidates) ===\")\n",
    "        for key, runs in kernel._benchmarked_configs.items():\n",
    "            print(f\"\\nInput Key: {key}\")\n",
    "            sorted_runs = sorted(runs, key=lambda x: x[1])\n",
    "            for config, timing in sorted_runs:\n",
    "                print(f\"  Config: {config.kwargs}, warps: {config.num_warps}, time: {timing * 1e3:.3f} ms\")\n",
    "            best = sorted_runs[0]\n",
    "            print(f\" Best: {best[0].kwargs} (time: {best[1] * 1e3:.3f} ms)\")\n",
    "    else:\n",
    "        print(\"\\n WARNING Benchmark history (_benchmarked_configs) not found or empty. This might mean:\")\n",
    "        print(\" - The kernel has not been invoked yet.\")\n",
    "        print(\" - Triton internals changed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d38e30-9736-49cb-8ba8-f257e36edbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_autotune_cache(attn_fwd, \"attn_fwd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec96d79-50fe-4814-a668-27c95ceaa04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vllm_tuned_times = []\n",
    "\n",
    "for seqlen in seqlens:\n",
    "    q = torch.randn(batch, nheads, seqlen, head_dim, device='cuda')\n",
    "    k = torch.randn(batch, nheads, seqlen, head_dim, device='cuda')\n",
    "    v = torch.randn(batch, nheads, seqlen, head_dim, device='cuda')\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    out_vllm_tuned = run_vllm_flash_attention(q, k, v, seqlen)\n",
    "    torch.cuda.synchronize()\n",
    "    tuned_time = time.time() - start\n",
    "    vllm_tuned_times.append(tuned_time)\n",
    "    print(f\"Seqlen={seqlen}: Tuned vLLM CUDA={tuned_time:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7e9593-48a3-42f0-a94d-6a45a4388354",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'SeqLen':>8} | {'PyTorch Time (s)':>18} | {'vLLM Tuned Time (s)':>20} | {'Speedup (PyTorch/vLLM)':>24}\")\n",
    "print(\"-\" * 75)\n",
    "for seqlen, pt_time, tuned_time in zip(seqlens, pytorch_times, vllm_tuned_times):\n",
    "    speedup = pt_time / tuned_time\n",
    "    print(f\"{seqlen:8} | {pt_time:18.6f} | {tuned_time:20.6f} | {speedup:24.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760c400d-2815-4323-9bd1-31a34dcba249",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, config in attn_fwd.cache.items():\n",
    "    print(f\"Best config for {key}: {config.kwargs}, num_warps={config.num_warps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627a2f59-96ae-4994-854e-b1c9259529bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(seqlens, pytorch_times, label=\"PyTorch SDPA (CUDA)\")\n",
    "plt.plot(seqlens, vllm_times, label=\"vLLM (Original)\")\n",
    "plt.plot(seqlens, vllm_tuned_times, label=\"vLLM (Autotuned)\")\n",
    "plt.xlabel(\"Sequence Length\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.title(\"Flash Attention Benchmark: PyTorch vs vLLM (Before & After Autotune)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd225668-944f-4058-a20e-973188c7442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(seqlens, pytorch_times, label=\"PyTorch SDPA (CUDA)\")\n",
    "plt.plot(seqlens, vllm_tuned_times, label=\"vLLM (Autotuned)\")\n",
    "plt.xlabel(\"Sequence Length\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.title(\"Flash Attention Benchmark: PyTorch vs vLLM (After Autotune)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
